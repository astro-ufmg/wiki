{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"2025/12/15/introdu%C3%A7%C3%A3o-a-mpi-e-petsc-em-python/","title":"Introdu\u00e7\u00e3o a MPI e PETSC em Python","text":"<p>Esta \u00e9 uma breve introdu\u00e7\u00e3o a utiliza\u00e7\u00e3o do MPI e o PETSC em programas escritos em Python. O material \u00e9 escrito em forma de exemplos de complexidade incremental. Cada caso procura explorar da forma mais simples possivel como alguns aspectos fundamentais destas duas bibliotecas est\u00e3o embutidos no c\u00f3digo. Assume-se que o leitor tenha instalado os pacotes mpi4py e petsc4py. </p> <p>Nota</p> <p>O <code>mpi4py</code> requer a instala\u00e7\u00e3o de alguma distribui\u00e7\u00e3o do MPI (OpenMPI, MPICH, ...). Se a instala\u00e7\u00e3o do <code>petsc4py</code> esta utilizando a vers\u00e3o do PETSc j\u00e1 presente no sistema, \u00e9 preciso certificar que tal vers\u00e3o foi linkada a mesma vers\u00e3o do MPI que esta sendo utilizada pelo mpi4py. Caso contr\u00e1rio, erros estranhos referentes a m\u00e1 decodifica\u00e7\u00e3o de mensagens do MPI podem ocorrer.</p>"},{"location":"2025/12/15/introdu%C3%A7%C3%A3o-a-mpi-e-petsc-em-python/#um-hello-world","title":"Um hello world","text":"<p>Em sua forma mais simples, o MPI consiste em rodar v\u00e1rias r\u00e9plicas do mesmo programa e executar chamadas em sua biblioteca quando quisermos que estes processos comuniquem. Neste caso simples (mas assustadoramente comum) cada instancia se diferencia por seu rank, um n\u00famero inteiro maior ou igual a zero.</p> <p>00-mpi-hello-world.py<pre><code>from mpi4py import MPI\n\nrank = MPI.COMM_WORLD.Get_rank()\nsize = MPI.COMM_WORLD.Get_size()\n\nprint(f\"Hello from {rank} out of {size}\")\n</code></pre> Rodando o programa com 2 mpi slots, resulta: <pre><code>$ mpirun -n 2 python 00-mpi-hello-world.py\nHello from 1 out of 2\nHello from 0 out of 2\n</code></pre></p> <p>Neste caso nenhuma comunica\u00e7\u00e3o foi feita entre os programas. Eles s\u00f3 iniciaram, escreveram na tela e sairam.</p> <p>Nota</p> <p>No MPI, \u00e9 possivel ter grupos de processos que tomam acoes coletivas separadamente, como se cada grupo fosse uma sala de chat. O mais comum \u00e9 ter um \u00fanico grupo de processos que rodam r\u00e9plicas de um mesmo programa, com caminhos de execuc\u00e3o bem parecios e realizam operac\u00f5es entre si. O grupo ou comunicador primario \u00e9 referenciado por <code>COMM_WORLD</code>. </p>"},{"location":"2025/12/15/introdu%C3%A7%C3%A3o-a-mpi-e-petsc-em-python/#caminhos-de-execucao-diferentes","title":"Caminhos de execu\u00e7\u00e3o diferentes","text":"<p>As vezes, queremos que processos diferentes realizem tarefas diferentes, como por exemplo, escrever um arquivo ou gerar um grafico serialmente. Tendo o rank atual em m\u00e3os, isto \u00e9 f\u00e1cil como escrever uma condicional:</p> 01-mpi-branches.py<pre><code>from mpi4py import MPI\nfrom time import sleep\n\nrank = MPI.COMM_WORLD.Get_rank()\n\ndef work():\n    sleep(1.0)\n    print(f\"Done working {rank}\")\n\ndef work_more():\n    sleep(2.0)\n    print(f\"Done working {rank}\")\n\nif rank != 0:\n    work()\nelse:\n    work_more()\n</code></pre> <pre><code>$ mpirun -n 2 python 01-mpi-branches.py\nDone working 1\nDone working 0\n</code></pre> <p>Dependendo do rank da instancia que estamos, o programa pode tomar caminhos de execuc\u00e3o distintos. Para evitar condicoes de corrida na escrita ou exportacao de dados, \u00e9 comum sincronizar os dados para uma unica instancia e realizar as operacoes somente dela.</p>"},{"location":"2025/12/15/introdu%C3%A7%C3%A3o-a-mpi-e-petsc-em-python/#basico-de-sincronizacao","title":"B\u00e1sico de sincroniza\u00e7\u00e3o","text":"<p>A princ\u00edpio, cada inst\u00e2ncia do MPI roda em paralelo (n\u00e3o \u00e9 necessariamente assim) o que \u00e9 um problema quando precisamos nos certificar que alguma parte do c\u00f3digo seja executada por um \u00fanico rank por vez. Neste caso, podemos utilizar uma primitiva de sincroniza\u00e7\u00e3o chamada Mutex:</p> 02-mpi-mutex.py<pre><code>from mpi4py import MPI\nfrom mpi4py.util import sync\nfrom time import sleep\n\ncomm = MPI.COMM_WORLD\nrank = comm.Get_rank()\n\ndef print_list():\n    for i in range(5):\n        print(f\"[rank {rank}] item {i}\")\n        sleep(0.1)\n    print(f\"[rank {rank}] done\")\n\nmutex = sync.Mutex(comm=comm)\n\nprint_list()\n\nwith mutex:\n    print_list()\n</code></pre> <pre><code>$ mpirun -n 2 python 02-mpi-mutex.py\n[rank 1] item 0\n[rank 0] item 0\n[rank 0] item 1\n[rank 1] item 1\n[rank 0] item 2\n[rank 1] item 2\n[rank 1] item 3\n[rank 0] item 3\n[rank 0] item 4\n[rank 1] item 4\n[rank 1] done\n[rank 0] done\n[rank 1] item 0\n[rank 1] item 1\n[rank 1] item 2\n[rank 1] item 3\n[rank 1] item 4\n[rank 1] done\n[rank 0] item 0\n[rank 0] item 1\n[rank 0] item 2\n[rank 0] item 3\n[rank 0] item 4\n[rank 0] done\n</code></pre> <p>Mutex (que significa Mutual exclusive e as vezes \u00e9 chamado de lock) \u00e9 uma forma de garantir que s\u00f3 uma instancia tenha acesso a algum recurso por vez. Note que na primeira chamada do <code>print_list()</code> as saidas intercalam entre os dois processos. Quando a funcao \u00e9 chamada ao adiquirir o mutex, um processo espera o outro terminar.</p> <p>Esta \u00e9 a forma mais facil de evitar condicoes de corrida.</p> <p>Nota</p> <p>Usu\u00e1rios do MPI em linguagens de mais baixo n\u00edvel, que interagem diretamente com a biblioteca do MPI v\u00e3o perceber que n\u00e3o existe um objeto tal como o Mutex. A interface do <code>mpi4py</code> prov\u00e9m esta e algumas outras primitivas de sincroniza\u00e7\u00e3o constru\u00eddas em cima das primitivas de troca de mensagem do MPI.</p>"},{"location":"2025/12/15/introdu%C3%A7%C3%A3o-a-mpi-e-petsc-em-python/#particionando-vetores-espacialment-no-petsc","title":"Particionando vetores espacialment no PETSc","text":"<p>Movendo agora para um exemplo com o petsc4py, vejamos como particionar espacialmente uma grade em diferentes processos do MPI.  </p> 03-petsc-global-local.py<pre><code>from mpi4py.util import sync\nfrom petsc4py import PETSc\n\ncomm = PETSc.COMM_WORLD\nrank = comm.getRank()\n\nn = 10\ndm = PETSc.DMDA().create([n], dof=1, stencil_width=1, comm=comm)\n\nglobal_vec = dm.createGlobalVector() \nlocal_vec = dm.createLocalVector()\n\nglobal_vec.set(rank)\nglobal_vec.assemble()\n\ndm.globalToLocal(global_vec, local_vec)\n\nmutex = sync.Mutex(comm = comm.tompi4py())\n\nwith mutex:\n    print(f\"rank {rank}:\")\n    print(\"  global:\", global_vec.getArray())\n    print(\"  local :\", local_vec.getArray())\n ```\n\n`DMDA` \u00e9 o nome dado a um objeto do PETSc que permite codificar informa\u00e7\u00f5es de conectividade de uma grade estruturada (quadrilateral). Utilizamos este objeto posteriormente pra criar vetores e realizar comunica\u00e7\u00f5es coletivas facilitando a parti\u00e7\u00e3o de um domn\u00ednio f\u00edsico entre diferentes processos. \n\n ```\n$ mpirun -n 2 python 03-petsc-global-local.py\nrank 1:\n  global: [1. 1. 1. 1. 1.]\n  local : [0. 1. 1. 1. 1. 1.]\nrank 0:\n  global: [0. 0. 0. 0. 0.]\n  local : [0. 0. 0. 0. 0. 1.]\n</code></pre> <p>Na nomeclatura do PETSc, global \u00e9 uma fatia (local) do vetor em questao, enquanto local \u00e9 esta fatia com os ghost points. Note que: 1. Com <code>n = 10</code> em <code>-n 2</code> o <code>global_vec</code> tem 5 pontos enquanto o <code>local_vec</code> tem 6. Experimente mudar o valor de <code>stencil_width</code>. 2. <code>global_vec.set(rank)</code> enche o vetor com o valor do rank atual  3. Ao chamar <code>dm.globalToLocal(global_vec, local_vec)</code> sincronizamos o valor dos ghost points entre os processos.</p> <p>Nota</p> <p>As interfaces <code>DM*</code> n\u00e3o s\u00e3o as mais primitivas no PETSc. Um vetor criado sem a utiliza\u00e7\u00e3o de algum variante deste objeto <code>DM</code> ser\u00e1 trivialmente particionado entre os processos. Isto ocorre quando, por exemplo, queremos apenas resolver um problema de \u00e1lgebra linear sem embutir qualquer informa\u00e7\u00e3o sobre a conectividade dos nossos operadores.</p>"},{"location":"2025/12/15/introdu%C3%A7%C3%A3o-a-mpi-e-petsc-em-python/#exportando-os-vetores-para-uma-unica-instancia","title":"Exportando os vetores para uma \u00fanica inst\u00e2ncia","text":"<p>Corriqueiramente, queremos utilizar o vetor (ou campo) completo em uma tarefa que n\u00e3o pode ser realizada em paralelo. Exemplos disto s\u00e3o polotagem dos resultados e exporta\u00e7\u00e3o para um arquivo (\u00e9 poss\u00edvel mas n\u00e3o trivial realizar escritas em paralelo). Neste caso, \u00e9 comum reconstruir o vetor completo em um \u00fanico processo, comumente o com rank zero (ele costumar se o mais lento). </p> 04-gather-on-zero.py<pre><code>from mpi4py.util import sync\nfrom petsc4py import PETSc\n\ncomm = PETSc.COMM_WORLD\nrank = comm.getRank()\n\nn = 10\ndm = PETSc.DMDA().create([n], dof=1, stencil_width=1, comm=comm)\n\nglobal_vec = dm.createGlobalVector()\nlocal_vec = dm.createLocalVector()\n\nglobal_vec.set(rank)\nglobal_vec.assemble()\n\ndm.globalToLocal(global_vec, local_vec)\n\nif rank == 0:\n    vec_zero = PETSc.Vec().createSeq(global_vec.getSize(), comm=PETSc.COMM_SELF)\nelse:\n    vec_zero = PETSc.Vec().createSeq(0, comm=PETSc.COMM_SELF)\n\nscatter, _ = PETSc.Scatter().toZero(global_vec)\nscatter.scatter(\n    global_vec, vec_zero, PETSc.InsertMode.INSERT, PETSc.ScatterMode.FORWARD\n)\n\nprint(f\"[rank {rank}]: \", vec_zero.getArray())\n</code></pre> <p>Note que o <code>vec_zero</code> tem tipos diferentes em ranks diferentes. Isto evita critar uma copia do vetor global em cada rank. Sobre a comunica\u00e7\u00e3o, a fun\u00e7\u00e3o <code>PETSc.Scatter().toZero(...)</code> retorna uma tupla com dois contextos de scatter: global para local e local para local. Utilizamos o global para local.</p> <pre><code>$ mpirun -n 2 python 04-gather-on-zero.py\n[rank 1]:  []\n[rank 0]:  [0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n</code></pre> <p>Note que o vetor <code>vec_zero</code> n\u00e3o ocupa mem\u00f3ria no rank 1. Se quisessemos ser super eficientes sobre memoria, poder\u00edamos receber, no rank zero, cada peda\u00e7o de um vetor sequencialmente, e realizar a opera\u00e7\u00e3o desejada. Isto evitaria a situa\u00e7\u00e3o em que o programa por completo teria dois campos inteiros alocatos em um \u00fanico instante: \\(N\\) peda\u00e7os distribu\u00eddos em \\(N\\) ranks e um inteiro do rank zero.</p> <p>Um coment\u00e1rio final sobre a diferen\u00e7a de nomeclatura entre as duas bibliotecas. No PETSc scatter s\u00e3o opera\u00e7\u00f5es genericas que redistribuem dados entre os processos. Na nomeclatura tradicional, por\u00e9m, scatter se refere ao caso em que enviamos uma informa\u00e7\u00e3o da instancia local para as outras e gather \u00e9 quando recebemos. As especifica\u00e7\u00e3o do MPI tamb\u00e9m disponibiliza algumas opera\u00e7\u00f5es que combinam as duas, chamadas de reduce. Um reduce pode somar todos os vetores distribuidos elemento a elemento ou tirar o m\u00e1ximo, por exemplo.</p>"},{"location":"2025/12/15/postando-no-blog-da-astro/","title":"Postando no blog da astro","text":"<p>Todos queremos compartilhar o que sabemos, mas raramente temos a oportunidade de estarmos no mesmo ambiente, no mesmo instante e ou com a mesma disponibilidade. A solu\u00e7\u00e3o para isto \u00e9 o blog da astro!</p> <p>O blog \u00e9 feito utilizando o mkdocs e o Material for mkdocs. Falando forma simples, estes programas convertem markdown em p\u00e1ginas da web, como esta! Tanto os arquivos markdown quanto as p\u00e1ginas geradas s\u00e3o hospedados em um reposit\u00f3rio da astro-ufmg. Para criar ou editar os posts, voce precisa clonar o reposit\u00f3rio localmente:</p> <pre><code>$ git clone git@github.com:astro-ufmg/wiki.git\n</code></pre> <p>Ou usando https se voce preferir (a autentica\u00e7\u00e3o por chaves com SSH \u00e9 bem mais f\u00e1cil). Mudan\u00e7as no blog precisam ser submetidas primeiramente ao branch draft. Vamos ent\u00e3o trocar localmente para este branch:</p> <pre><code>$ git checkout draft\nSwitched to branch 'draft'\nYour branch is up to date with 'origin/draft'.\n</code></pre>"},{"location":"2025/12/15/postando-no-blog-da-astro/#ajeitando-o-ambiente","title":"Ajeitando o ambiente","text":"<p>Como dito antes, h\u00e1 scripts que convertem o conte\u00fado em markdown nestas p\u00e1ginas bonitas e arquivam os posts. Para facilitar o proceso, esta incluso no reposit\u00f3rio um <code>Makefile</code> que facilita a instala\u00e7\u00e3o deste script. Para criar um enviroment com as dependencias necess\u00e1rias, execute na raiz do reposit\u00f3rio: </p> <pre><code>$ make setup\n</code></pre> <p>Isto utilizar\u00e1 o pip (ou o <code>uv</code> se instalado) para criar um ambiente em <code>.venv</code> na raiz do reposit\u00f3rio local com as dependencias necess\u00e1rias. Para vizualizar o site localmente enquanto edita, use:</p> <pre><code>$ make serve \n</code></pre> <p>Toda vez que algum conte\u00fado novo for publicado, lembre-se de executar:</p> <pre><code>$ make build \n</code></pre>"},{"location":"2025/12/15/postando-no-blog-da-astro/#criando-um-post","title":"Criando um post","text":"<p>Listando os diret\u00f3rios relevantes, vemos as seguinte estrutura:</p> <pre><code>\u251c\u2500\u2500 content\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 authors.yml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 images\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 authors\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 ...\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 posts\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 ...\n</code></pre> <p>No <code>content/posts</code> voce pode criar um arquivo <code>.md</code> com o post que voce deseja criar. Suponhamos que o t\u00edtulo do post seja \"Baixando dados de aglomerados\", o arquivo deve se chamar <code>baixando-dados-de-aglomerados.md</code> (evitando acentos e caracteres especiais) por uma quest\u00e3o de organiza\u00e7\u00e3o. Todo arquivo de post precisa de um cabe\u00e7alho, veja por exemplo o cabe\u00e7alho deste post que voce esta lendo:</p> <pre><code>---\ntitle: Postando no blog da astro\nauthor: [lsmenicucci]\ndate: 2025-12-15\ncategories:\n  - meta\n---\n</code></pre> <p>O cabe\u00e7alho fica delimitado por <code>---</code>. Os nomes no campo <code>author</code> se referem a entradas no arquivo <code>content/authors.yml</code>, caso queira adicionar um autor, veja o arquivo, \u00e9 autoexplicativo. Agora \u00e9 so escrever depois do cabe\u00e7alho! Para que o post apresente uma vers\u00e3o resumida na lista de posts, coloque <code>&lt;!-- more --&gt;</code> quando voce quiser que o conte\u00fado abaixo fique restrito apenas a visualiza\u00e7\u00e3o \u00fanica do post. Algo do tipo:</p> <pre><code>Este par\u00e1grafo vai aparecer na lista de posts como um resumo!\n\n&lt;!-- more --&gt;\n\nAqui eu j\u00e1 preciso clicar em \"Ler mais\" para continuar.\n</code></pre> <p>Editado o post, lembre-se de gerar o site novamente e commitar para o branch draft:</p> <pre><code>$ make build\n$ git add content/posts/baixando-dados-de-aglomerados.md\n$ git add docs \n$ git commit -m \"Novo post: Baixando dados de aglomerados\"\n$ git push\n</code></pre> <p>Se voce n\u00e3o estiver editando um post simultaneamente a outra pessoa, o commit deve ser naturalmente aceito. Note que adicionamos o diret\u00f3rio <code>docs</code> onde as paginas da web foram geradas. \u00c9 elas que est\u00e3o sendo exibidas aqui!</p>"},{"location":"archive/2025/","title":"2025","text":""},{"location":"category/python/","title":"python","text":""},{"location":"category/mpi/","title":"mpi","text":""},{"location":"category/meta/","title":"meta","text":""}]}